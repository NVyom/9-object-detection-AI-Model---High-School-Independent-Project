{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446c3698-e803-4b5b-8f51-c8b1a918d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import random\n",
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b8468f-fba8-44be-90fb-0c81d5f28c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.12).\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.12).\n"
     ]
    }
   ],
   "source": [
    "fruit_path = kagglehub.dataset_download(\"raghavrpotdar/fresh-and-stale-images-of-fruits-and-vegetables\")\n",
    "\n",
    "food_path = kagglehub.dataset_download(\"anthonytherrien/image-classification-32-classes-food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25948779-5695-4835-ada4-5e003e8480ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is D2D6-E9FB\n",
      "\n",
      " Directory of C:\\Users\\PC\\.cache\\kagglehub\\datasets\\raghavrpotdar\\fresh-and-stale-images-of-fruits-and-vegetables\\versions\\1\n",
      "\n",
      "29-05-2025  22:10    <DIR>          .\n",
      "29-05-2025  22:10    <DIR>          ..\n",
      "29-05-2025  22:10    <DIR>          fresh_apple\n",
      "29-05-2025  22:10    <DIR>          fresh_banana\n",
      "29-05-2025  22:10    <DIR>          fresh_bitter_gourd\n",
      "29-05-2025  22:10    <DIR>          fresh_capsicum\n",
      "29-05-2025  22:10    <DIR>          fresh_orange\n",
      "29-05-2025  22:10    <DIR>          fresh_tomato\n",
      "29-05-2025  22:10               180 ImageLabels.txt\n",
      "29-05-2025  22:10    <DIR>          stale_apple\n",
      "29-05-2025  22:10    <DIR>          stale_banana\n",
      "29-05-2025  22:10    <DIR>          stale_bitter_gourd\n",
      "29-05-2025  22:10    <DIR>          stale_capsicum\n",
      "29-05-2025  22:10    <DIR>          stale_orange\n",
      "29-05-2025  22:10    <DIR>          stale_tomato\n",
      "               1 File(s)            180 bytes\n",
      "              14 Dir(s)  133,106,348,032 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls \"$fruit_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d26a0d-4959-4bdc-8113-586ec3df2c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is D2D6-E9FB\n",
      "\n",
      " Directory of C:\\Users\\PC\\.cache\\kagglehub\\datasets\\anthonytherrien\\image-classification-32-classes-food\\versions\\1\n",
      "\n",
      "30-05-2025  11:16    <DIR>          .\n",
      "30-05-2025  11:16    <DIR>          ..\n",
      "30-05-2025  11:17    <DIR>          image\n",
      "               0 File(s)              0 bytes\n",
      "               3 Dir(s)  133,106,348,032 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls \"$food_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6295208b-cf20-443a-9a38-f10610a07cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the dataset directory:\n",
      "['alfredo', 'broccoli', 'brownie', 'cake', 'carrot', 'cereal', 'cheese', 'chicken', 'chocolate', 'coffee', 'cookie', 'corn', 'couscous', 'crab', 'donut', 'egg', 'fajitas', 'fries', 'grilledcheese', 'hotdog', 'icecream', 'macncheese', 'nachos', 'nuggets', 'rice', 'salad', 'salmon', 'shrimp', 'soup', 'steak', 'sushi', 'tartare']\n"
     ]
    }
   ],
   "source": [
    "print(\"Files in the dataset directory:\")\n",
    "print(os.listdir(r'C:\\Users\\PC\\.cache\\kagglehub\\datasets\\anthonytherrien\\image-classification-32-classes-food\\versions\\1\\image'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8ee97b-bee3-47c9-99ac-50445d88b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_path = r\"C:\\Users\\PC\\.cache\\kagglehub\\datasets\\anthonytherrien\\image-classification-32-classes-food\\versions\\1\\image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb14936-0107-4ab9-a9ce-5bc21fa4654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dir = \"visionXXXX\"\n",
    "\n",
    "os.makedirs(os.path.join(subset_dir), exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(subset_dir, \"train\")\n",
    "val_dir = os.path.join(subset_dir, \"val\")\n",
    "\n",
    "\n",
    "os.makedirs(os.path.join(train_dir, \"apple\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"banana\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"orange\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"tomato\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"capsicum\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"bitter gourd\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"broccoli\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"carrot\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"rice\"), exist_ok=True)\n",
    "\n",
    "os.makedirs(os.path.join(val_dir, \"apple\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"banana\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"orange\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"tomato\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"capsicum\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"bitter gourd\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"broccoli\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"carrot\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"rice\"), exist_ok=True)\n",
    "\n",
    "orange_path = os.path.join(fruit_path, r\"C:\\Users\\PC\\.cache\\kagglehub\\datasets\\raghavrpotdar\\fresh-and-stale-images-of-fruits-and-vegetables\\versions\\1\", \"fresh_orange\")\n",
    "banana_path = os.path.join(fruit_path, r\"C:\\Users\\PC\\.cache\\kagglehub\\datasets\\raghavrpotdar\\fresh-and-stale-images-of-fruits-and-vegetables\\versions\\1\", \"fresh_banana\")\n",
    "apple_path = os.path.join(fruit_path, r\"C:\\Users\\PC\\.cache\\kagglehub\\datasets\\raghavrpotdar\\fresh-and-stale-images-of-fruits-and-vegetables\\versions\\1\", \"fresh_apple\")\n",
    "cap_path = os.path.join(fruit_path, r\"C:\\Users\\PC\\.cache\\kagglehub\\datasets\\raghavrpotdar\\fresh-and-stale-images-of-fruits-and-vegetables\\versions\\1\", \"fresh_capsicum\")\n",
    "tom_path = os.path.join(fruit_path, r\"C:\\Users\\PC\\.cache\\kagglehub\\datasets\\raghavrpotdar\\fresh-and-stale-images-of-fruits-and-vegetables\\versions\\1\", \"fresh_tomato\")\n",
    "bit_path = os.path.join(fruit_path, r\"C:\\Users\\PC\\.cache\\kagglehub\\datasets\\raghavrpotdar\\fresh-and-stale-images-of-fruits-and-vegetables\\versions\\1\", \"fresh_bitter_gourd\")\n",
    "broc_path = os.path.join(food_path, r'C:\\Users\\PC\\.cache\\kagglehub\\datasets\\anthonytherrien\\image-classification-32-classes-food\\versions\\1\\image', \"broccoli\")\n",
    "car_path = os.path.join(food_path, r'C:\\Users\\PC\\.cache\\kagglehub\\datasets\\anthonytherrien\\image-classification-32-classes-food\\versions\\1\\image', \"carrot\")\n",
    "rice_path = os.path.join(food_path, r'C:\\Users\\PC\\.cache\\kagglehub\\datasets\\anthonytherrien\\image-classification-32-classes-food\\versions\\1\\image', \"rice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec540a14-ceb8-4fe9-a87c-076a78be2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "orange_img = sorted([f for f in os.listdir(orange_path) if f.endswith(\".png\")])\n",
    "banana_img = sorted([f for f in os.listdir(banana_path) if f.endswith(\".png\")])\n",
    "apple_img = sorted([f for f in os.listdir(apple_path) if f.endswith(\".png\")])\n",
    "tom_img = sorted([f for f in os.listdir(tom_path) if f.endswith(\".jpg\")])\n",
    "cap_img = sorted([f for f in os.listdir(cap_path) if f.endswith(\".jpg\")])\n",
    "bit_img = sorted([f for f in os.listdir(bit_path) if f.endswith(\".jpg\")])\n",
    "broc_img = sorted([f for f in os.listdir(broc_path) if f.endswith(\".png\")])\n",
    "car_img = sorted([f for f in os.listdir(car_path) if f.endswith(\".png\")])\n",
    "rice_img = sorted([f for f in os.listdir(rice_path) if f.endswith(\".png\")])\n",
    "\n",
    "for img in orange_img[:500]:\n",
    "    shutil.copy(os.path.join(orange_path, img), os.path.join(train_dir, \"orange\"))\n",
    "for img in orange_img[-250:]:\n",
    "    shutil.copy(os.path.join(orange_path, img), os.path.join(val_dir, \"orange\"))\n",
    "\n",
    "for img in banana_img[:500]:\n",
    "    shutil.copy(os.path.join(banana_path, img), os.path.join(train_dir, \"banana\"))\n",
    "for img in banana_img[-250:]:\n",
    "    shutil.copy(os.path.join(banana_path, img), os.path.join(val_dir, \"banana\"))\n",
    "\n",
    "for img in apple_img[:500]:\n",
    "    shutil.copy(os.path.join(apple_path, img), os.path.join(train_dir, \"apple\"))\n",
    "for img in apple_img[-250:]:\n",
    "    shutil.copy(os.path.join(apple_path, img), os.path.join(val_dir, \"apple\"))\n",
    "\n",
    "for img in tom_img[:500]:\n",
    "    shutil.copy(os.path.join(tom_path, img), os.path.join(train_dir, \"tomato\"))\n",
    "for img in tom_img[-250:]:\n",
    "    shutil.copy(os.path.join(tom_path, img), os.path.join(val_dir, \"tomato\"))\n",
    "\n",
    "for img in cap_img[:500]:\n",
    "    shutil.copy(os.path.join(cap_path, img), os.path.join(train_dir, \"capsicum\"))\n",
    "for img in cap_img[-250:]:\n",
    "    shutil.copy(os.path.join(cap_path, img), os.path.join(val_dir, \"capsicum\"))\n",
    "\n",
    "for img in bit_img[:200]:\n",
    "    shutil.copy(os.path.join(bit_path, img), os.path.join(train_dir, \"bitter gourd\"))\n",
    "for img in bit_img[-100:]:\n",
    "    shutil.copy(os.path.join(bit_path, img), os.path.join(val_dir, \"bitter gourd\"))\n",
    "\n",
    "for img in broc_img[:175]:\n",
    "    shutil.copy(os.path.join(broc_path, img), os.path.join(train_dir, \"broccoli\"))\n",
    "for img in broc_img[-88:]:\n",
    "    shutil.copy(os.path.join(broc_path, img), os.path.join(val_dir, \"broccoli\"))\n",
    "\n",
    "for img in car_img[:180]:\n",
    "    shutil.copy(os.path.join(car_path, img), os.path.join(train_dir, \"carrot\"))\n",
    "for img in car_img[-108:]:\n",
    "    shutil.copy(os.path.join(car_path, img), os.path.join(val_dir, \"carrot\"))\n",
    "\n",
    "for img in rice_img[:180]:\n",
    "    shutil.copy(os.path.join(rice_path, img), os.path.join(train_dir, \"rice\"))\n",
    "for img in rice_img[-104:]:\n",
    "    shutil.copy(os.path.join(rice_path, img), os.path.join(val_dir, \"rice\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590ef743-4d4f-4232-bdb1-5b2cc01bd045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a train/val subset in: visionXXXX\n"
     ]
    }
   ],
   "source": [
    "print(\"Created a train/val subset in:\", subset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beaa3f6f-03a6-433a-af1a-ad3b9a8cbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomRotation(60),\n",
    "    transforms.RandomHorizontalFlip(p=0.8),\n",
    "    transforms.RandomVerticalFlip(p=0.6),  # Try vertical flip\n",
    "    transforms.RandomAffine(degrees=40, translate=(0.6, 0.6)),  # Slight translations\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # More aggressive cropping\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose ([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c205ee4-0cb9-49e5-b195-57eeaccda30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5deb29e6-298c-422e-af57-13ecf8244e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['apple', 'banana', 'bitter gourd', 'broccoli', 'capsicum', 'carrot', 'orange', 'rice', 'tomato']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_dataset.classes\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d198373-0b7c-4d1d-a937-d334eb2bbaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda as device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device, \"as device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbd38b09-4eda-49c4-84f9-aad83107fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "995f5f38-ea46-4a08-9b5e-a874e7f58e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "num_features = model.classifier[1].in_features\n",
    "# Modify the last layer for 5 classes\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 9),\n",
    "    nn.Dropout(0.4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be9066fc-c7f2-4d37-a18a-3b422bdadc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1.0, 1.0, 1.0,1.0,1.0,1.0,1.0,1.0,1.0]).to(device)  \n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-6, weight_decay = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "933b28af-e968-4259-a059-af0aaaa4bbc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 2.123, Train Acc: 0.190, Val Loss: 2.061, Val Acc: 0.205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Train Loss: 1.999, Train Acc: 0.271, Val Loss: 1.903, Val Acc: 0.328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Train Loss: 1.860, Train Acc: 0.368, Val Loss: 1.773, Val Acc: 0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Train Loss: 1.761, Train Acc: 0.427, Val Loss: 1.635, Val Acc: 0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Train Loss: 1.673, Train Acc: 0.464, Val Loss: 1.516, Val Acc: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Train Loss: 1.569, Train Acc: 0.502, Val Loss: 1.376, Val Acc: 0.839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Train Loss: 1.493, Train Acc: 0.519, Val Loss: 1.282, Val Acc: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Train Loss: 1.416, Train Acc: 0.551, Val Loss: 1.168, Val Acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Train Loss: 1.337, Train Acc: 0.565, Val Loss: 1.056, Val Acc: 0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Train Loss: 1.288, Train Acc: 0.578, Val Loss: 0.990, Val Acc: 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Train Loss: 1.245, Train Acc: 0.591, Val Loss: 0.908, Val Acc: 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Train Loss: 1.210, Train Acc: 0.583, Val Loss: 0.844, Val Acc: 0.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Train Loss: 1.183, Train Acc: 0.580, Val Loss: 0.777, Val Acc: 0.976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;66;03m# Check for early stopping\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     11\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Training Phase\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     15\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2356\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2344\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2346\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2347\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2348\u001b[0m         )\n\u001b[0;32m   2349\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2350\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2351\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2352\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2353\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2354\u001b[0m         )\n\u001b[1;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training Phase\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # Calculate training loss and accuracy\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = running_corrects / total\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Metrics calculation\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(preds == labels).item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        # Calculate validation loss and accuracy\n",
    "        val_loss = val_running_loss / val_total\n",
    "        val_acc = val_running_corrects / val_total\n",
    "\n",
    "        # Logging the results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f}, \"\n",
    "              f\"Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "\n",
    "        # Update the scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        # Check for early stopping\n",
    " \n",
    "train(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b585f541-afc8-4386-9c9e-2066dd29acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'new_fruit_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
